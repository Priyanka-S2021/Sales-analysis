# -*- coding: utf-8 -*-
"""amazon sales.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CvZC7s5UUXNb9d25O5RbcVh6xu_0kFHX
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

df1= pd.read_csv('/content/Amazon Sales data (1).csv')
df1.head()



df1.describe()

df1.shape

df1.groupby('Region')['Region'].agg('count')

df1.groupby('Sales Channel')['Sales Channel'].agg('count')

df1.groupby('Item Type')['Item Type'].agg('count')

df2=df1.drop(['Country','Order Priority','Order ID','Sales Channel','Ship Date','Item Type'], axis= 'columns')

df2.head()

df2.isnull().sum()

df2.shape

df2[df2['Unit Price'] < 100]['Unit Price'].unique()

df2 = df2[df2['Unit Price'] >= 100]
df2 = df2[df2['Unit Cost'] >= 100]
df2.head()

df2['Order year'] = df2['Order Date'].apply(lambda x: int(x.split('/')[2]))
df2.head()

df3= df2.drop(['Order Date'],axis='columns')
print(df3)

df3['Unit Cost'].unique()

df3.shape

df3.reset_index()

df3['Profit']=(df3['Total Revenue']/df3['Total Cost'])
df3.head(50)

len(df4['Region'].unique())



import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# Assuming df1 is your DataFrame containing the data

# Drop unnecessary columns for this example
df3_ml = df1.drop(['Order Date', 'Ship Date', 'Order ID'], axis=1)

# Convert categorical columns to dummy variables (one-hot encoding)
df3_ml = pd.get_dummies(df3_ml, columns=['Region', 'Country', 'Item Type', 'Sales Channel', 'Order Priority'], drop_first=True)

# Split the data into features (X) and target variable (y)
X = df3_ml.drop('Total Profit', axis=1)
y = df3_ml['Total Profit']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Train a Linear Regression model
model = LinearRegression()
model.fit(X_train_scaled, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test_scaled)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f'Mean Squared Error: {mse:.2f}')
print(f'R-squared: {r2:.2f}')

df=df3.copy()

sns.pairplot(df, diag_kind='kde')
plt.show()

# Correlation heatmap
correlation_matrix = df.corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.show()

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
categorical_columns = ['Region']
for col in categorical_columns:
    df[col] = le.fit_transform(df[col])

X = df.drop('Total Profit', axis=1)
y = df['Total Profit']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Random Forest Regressor model
model = RandomForestRegressor()
model.fit(X_train, y_train)

# Predictions
y_pred = model.predict(X_test)

mse = mean_squared_error(y_test, y_pred)
print(f'Mean Squared Error: {mse}')

feature_importance = model.feature_importances_
feature_importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importance})
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

plt.figure(figsize=(10, 6))
sns.barplot(x='Importance', y='Feature', data=feature_importance_df)
plt.title('Feature Importance')
plt.show()